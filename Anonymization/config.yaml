# config.yaml

Train:
  tuning: True #True for hyperparamter training, false for single run-through
  batch_size: 128
  learning_rate: 0.001
  reg: 0.0005
  epochs: 20
  steps: [5, 15]
  warmup: 2
  momentum: 0.9
  dropout_rate: 0.5
  optimizer: Adam #Adam, SGD

network:
  model: SimpleModel # DropoutAndBatchnormModel, Dropoutmodel, SimpleModel

data:
  save_best: True

loss:
  loss_type: CE #CE

file_paths:
  train_file_path: train_cifar100.npz
  test_file_path: test_cifar100.npz

Anonymization:
  method: density_based #density_based, laplace, dp, hasing, pca, premutation, random
  eps: 1.2
  min_samples: 3
  noise_scale: 0.1

Anonymization_tuning:
  eps_tuning: [0.025, 0.05, 0.075, 0.1, 0.125, 0.15, 0.175, 0.2, 0.225, 0.25, 0.275, 0.30, 0.325, 0.35, 0.375, 0.40, 0.425, 0.45, 0.475, 0.50, 0.525, 0.55, 0.575, 0.60, 0625, 0.65, 0.70, 0.725, 0.75, 0.775, 0.80, 0.825, 0.85, 0.875, 0.9, 0.925, 0.95, 0.975, 1.0]
  min_samples_tuning: [2]
  noise_scale_tuning: [0.01]
