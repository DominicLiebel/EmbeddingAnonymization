{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "hu54j531w8wj",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1699969788179,
     "user_tz": -60,
     "elapsed": 6698,
     "user": {
      "displayName": "Dominic Liebel",
      "userId": "07688395430318574683"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-01-21T15:37:52.050099Z",
     "start_time": "2024-01-21T15:37:51.858944Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 3\n",
      "Original test embeddings:\n",
      "[[0.48 0.48 0.48]\n",
      " [0.85 0.85 0.85]\n",
      " [0.9  0.9  0.9 ]]\n",
      "\n",
      "Anonymized test embeddings:\n",
      "[[0.525 0.525 0.525]\n",
      " [0.85  0.85  0.85 ]\n",
      " [0.95  0.95  0.95 ]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [50] at index 0 does not match the shape of the indexed tensor [10, 5] at index 0",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[77], line 306\u001B[0m\n\u001B[1;32m    303\u001B[0m test_embeddings \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrandn(\u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m5\u001B[39m)\n\u001B[1;32m    304\u001B[0m train_embeddings,  cluster_labels, num_clusters \u001B[38;5;241m=\u001B[39m anonymize_embeddings_density_based(original_embeddings, eps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50\u001B[39m, min_samples\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, noise_scale\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.00\u001B[39m)\n\u001B[0;32m--> 306\u001B[0m test_embeddings_anonymized \u001B[38;5;241m=\u001B[39m \u001B[43manonymize_embeddings_density_based_test\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_embeddings\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcluster_labels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnoise_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.00\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    308\u001B[0m \u001B[38;5;66;03m# Visualize both original and anonymized embeddings\u001B[39;00m\n\u001B[1;32m    309\u001B[0m visualize_embeddings2(test_embeddings, test_embeddings_anonymized, title\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTest and Test Anonymized Embeddings\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[0;32mIn[77], line 213\u001B[0m, in \u001B[0;36manonymize_embeddings_density_based_test\u001B[0;34m(test_embeddings, train_cluster_labels, noise_scale, device)\u001B[0m\n\u001B[1;32m    211\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m label \u001B[38;5;129;01min\u001B[39;00m unique_labels:\n\u001B[1;32m    212\u001B[0m     cluster_indices \u001B[38;5;241m=\u001B[39m (train_cluster_labels \u001B[38;5;241m==\u001B[39m label)\n\u001B[0;32m--> 213\u001B[0m     anonymized_embeddings[cluster_indices] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m laplace_noise[cluster_indices]\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m anonymized_embeddings\n",
      "\u001B[0;31mIndexError\u001B[0m: The shape of the mask [50] at index 0 does not match the shape of the indexed tensor [10, 5] at index 0"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sys\n",
    "from random import randrange\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "def get_cluster_edges(cluster_embeddings):\n",
    "    min_values = np.min(cluster_embeddings, axis=0)\n",
    "    max_values = np.max(cluster_embeddings, axis=0)\n",
    "    return min_values, max_values\n",
    "\n",
    "\n",
    "def anonymize_embeddings(original_embeddings, eps, min_samples):\n",
    "    # Create a DBSCAN model\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "\n",
    "    # Fit the model to the original embeddings\n",
    "    labels = dbscan.fit_predict(original_embeddings)\n",
    "\n",
    "    # Get the unique cluster labels\n",
    "    unique_labels = np.unique(labels)\n",
    "    print(\"Number of clusters:\", len(unique_labels))\n",
    "\n",
    "    # Initialize an array to store the cluster edges\n",
    "    cluster_edges = []\n",
    "\n",
    "    # Calculate the edges for each cluster\n",
    "    for label in unique_labels:\n",
    "        cluster_mask = (labels == label)\n",
    "        cluster_embeddings = original_embeddings[cluster_mask]\n",
    "        min_values, max_values = get_cluster_edges(cluster_embeddings)\n",
    "        cluster_edges.append((min_values, max_values))\n",
    "\n",
    "\n",
    "\n",
    "    return cluster_edges\n",
    "\n",
    "\n",
    "def anonymize_test_embeddings(cluster_edges, test_embeddings):\n",
    "    anonymized_test_embeddings = []\n",
    "\n",
    "    for test_embedding in test_embeddings:\n",
    "        found_cluster = False\n",
    "\n",
    "        for cluster_edge in cluster_edges:\n",
    "            min_values, max_values = cluster_edge\n",
    "\n",
    "            if np.all(test_embedding >= min_values) and np.all(test_embedding <= max_values):\n",
    "                # Test embedding is within the cluster, use cluster coordinates\n",
    "                anonymized_test_embeddings.append((min_values + max_values) / 2)  # Centroid\n",
    "                found_cluster = True\n",
    "                break\n",
    "\n",
    "        if not found_cluster:\n",
    "            # Test embedding does not belong to any cluster, use original test embedding\n",
    "            anonymized_test_embeddings.append(test_embedding)\n",
    "\n",
    "    return np.array(anonymized_test_embeddings)\n",
    "\n",
    "# Larger set of original embeddings\n",
    "larger_original_embeddings = np.array([[0.45, 0.45, 0.45],\n",
    "                                       [0.9, 0.9, 0.9],\n",
    "                                       [0.5, 0.5, 0.5],\n",
    "                                       [0.7, 0.7, 0.7],\n",
    "                                       [1.0, 1.0, 1.0],\n",
    "                                       [0.6, 0.6, 0.6],\n",
    "                                       [0.8, 0.8, 0.8],\n",
    "                                       [0.55, 0.55, 0.55],\n",
    "                                       [0.95, 0.95, 0.95],\n",
    "                                       [0.75, 0.75, 0.75]])\n",
    "\n",
    "# Set DBSCAN parameters\n",
    "eps = 0.1  # Maximum distance between samples to be considered in the same neighborhood\n",
    "min_samples = 1  # Minimum number of samples required to form a dense region\n",
    "\n",
    "# Test embeddings\n",
    "test_embeddings = np.array([[0.48, 0.48, 0.48],\n",
    "                            [0.85, 0.85, 0.85],\n",
    "                            [0.9, 0.9, 0.9]])\n",
    "\n",
    "# Get cluster edges for larger embeddings\n",
    "cluster_edges = anonymize_embeddings(larger_original_embeddings, eps, min_samples)\n",
    "\n",
    "# Anonymize test embeddings\n",
    "anonymized_test_embeddings = anonymize_test_embeddings(cluster_edges, test_embeddings)\n",
    "\n",
    "# Print the original test embeddings and anonymized test embeddings\n",
    "print(\"Original test embeddings:\")\n",
    "print(test_embeddings)\n",
    "print(\"\\nAnonymized test embeddings:\")\n",
    "print(anonymized_test_embeddings)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def anonymize_embeddings_hashing(embeddings, precision=16):\n",
    "    \"\"\"\n",
    "    Anonymize embeddings using hashing with randomized salt.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings (torch.Tensor): Input embeddings to be anonymized.\n",
    "    - precision (int): Precision for rounding float values before hashing.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: Anonymized embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    # Round float values and convert to strings\n",
    "    rounded_embeddings_str = np.round(embeddings, precision).astype(str)\n",
    "\n",
    "    # Generate random salt for each embedding\n",
    "    salts = np.array([randrange(sys.maxsize) for _ in range(len(embeddings))])\n",
    "\n",
    "    # Apply the hash function to all embeddings with randomized salt\n",
    "    hashed_embeddings = np.empty(len(embeddings), dtype='int64')\n",
    "    for i, embedding in enumerate(rounded_embeddings_str):\n",
    "        try:\n",
    "            # Convert valid integers to int arrays\n",
    "            hashed_embeddings[i] = np.array(int(embedding), dtype='int64')\n",
    "        except ValueError:\n",
    "            # Keep invalid values as strings\n",
    "            hashed_embeddings[i] = embedding\n",
    "\n",
    "    # Add salt to hashed embeddings\n",
    "    hashed_embeddings += salts\n",
    "\n",
    "    return hashed_embeddings.astype(torch.long)\n",
    "\n",
    "def anonymize_embeddings_density_based(embeddings, eps=0.5, min_samples=5, noise_scale=0.01, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Anonymize embeddings using density-based clustering.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: PyTorch tensor or NumPy array, the original embeddings\n",
    "    - eps: float, maximum distance between two samples for one to be considered as in the neighborhood of the other\n",
    "    - min_samples: int, the number of samples in a neighborhood for a point to be considered as a core point\n",
    "    - noise_scale: float, scale parameter for Laplace noise\n",
    "    - device: str, device to place the noise tensor on (\"cpu\" or \"cuda\")\n",
    "\n",
    "    Returns:\n",
    "    - PyTorch tensor, anonymized embeddings\n",
    "    \"\"\"\n",
    "    if isinstance(embeddings, np.ndarray):\n",
    "        embeddings = torch.tensor(embeddings, dtype=torch.float32, device=device)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    embeddings = scaler.fit_transform(embeddings)\n",
    "\n",
    "    # Perform density-based clustering using DBSCAN\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples).fit(embeddings)\n",
    "\n",
    "    # Assign a cluster label to each data point\n",
    "    cluster_labels = db.labels_\n",
    "\n",
    "    # Calculate centroids\n",
    "    centroids = embeddings[db.core_sample_indices_]\n",
    "\n",
    "    # Generate Laplace noise\n",
    "    laplace_noise = np.random.laplace(scale=noise_scale, size=embeddings.shape)\n",
    "\n",
    "    # Add noise to non-centroid embeddings\n",
    "    anonymized_embeddings = embeddings.copy()\n",
    "    for index, label in enumerate(cluster_labels):\n",
    "        if label != -1:  # Non-centroid\n",
    "            anonymized_embeddings[index] += laplace_noise[index]\n",
    "\n",
    "    # Add centroids to anonymized embeddings\n",
    "    anonymized_embeddings[db.core_sample_indices_] = centroids + laplace_noise[db.core_sample_indices_]\n",
    "\n",
    "    # Count the number of clusters\n",
    "    num_clusters = len(np.unique(cluster_labels)) - (1 if -1 in np.unique(cluster_labels) else 0)\n",
    "\n",
    "    return anonymized_embeddings, cluster_labels, num_clusters\n",
    "\n",
    "def anonymize_embeddings_density_based_test(test_embeddings, train_cluster_labels, noise_scale=0.01, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Anonymize test embeddings using density-based clustering.\n",
    "\n",
    "    Parameters:\n",
    "    - test_embeddings: PyTorch tensor or NumPy array, the test embeddings\n",
    "    - train_cluster_labels: NumPy array, cluster labels for the train embeddings\n",
    "    - noise_scale: float, scale parameter for Laplace noise\n",
    "    - device: str, device to place the noise tensor on (\"cpu\" or \"cuda\")\n",
    "\n",
    "    Returns:\n",
    "    - PyTorch tensor, anonymized test embeddings\n",
    "    \"\"\"\n",
    "    if isinstance(test_embeddings, np.ndarray):\n",
    "        test_embeddings = torch.tensor(test_embeddings, dtype=torch.float32, device=device)\n",
    "\n",
    "    # Apply noise to the test embeddings according to the train clusters\n",
    "    laplace_noise = torch.tensor(np.random.laplace(scale=noise_scale, size=test_embeddings.shape), dtype=torch.float32)\n",
    "\n",
    "    unique_labels = np.unique(train_cluster_labels)\n",
    "    anonymized_embeddings = test_embeddings.clone()\n",
    "\n",
    "    for label in unique_labels:\n",
    "        cluster_indices = (train_cluster_labels == label)\n",
    "        anonymized_embeddings[cluster_indices] += laplace_noise[cluster_indices]\n",
    "\n",
    "    return anonymized_embeddings\n",
    "\n",
    "\n",
    "def anonymize_embeddings_pca(embeddings, n_components=2):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    anonymized_embeddings = torch.tensor(pca.fit_transform(embeddings), dtype=torch.float32)\n",
    "    return anonymized_embeddings\n",
    "\n",
    "def reconstruct_embeddings_pca(anonymized_embeddings, pca_model):\n",
    "    reconstructed_embeddings = torch.tensor(pca_model.inverse_transform(anonymized_embeddings), dtype=torch.float32)\n",
    "    return reconstructed_embeddings\n",
    "\n",
    "\n",
    "def visualize_embeddings2(original_embeddings, anonymized_embeddings, title='Embeddings Visualization'):\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Plot original embeddings in blue\n",
    "    ax.scatter(original_embeddings[:, 0], original_embeddings[:, 1], original_embeddings[:, 2], c='b', marker='o', label='Original Embeddings', s=50)\n",
    "\n",
    "    # Plot anonymized embeddings in red\n",
    "    ax.scatter(anonymized_embeddings[:, 0], anonymized_embeddings[:, 1], anonymized_embeddings[:, 2], c='r', marker='o', label='Anonymized Embeddings')\n",
    "\n",
    "    # Connect each original point to its corresponding anonymized point\n",
    "    for orig, anon in zip(original_embeddings, anonymized_embeddings):\n",
    "        ax.plot([orig[0], anon[0]], [orig[1], anon[1]], [orig[2], anon[2]], color='gray', linestyle='--', linewidth=0.5)\n",
    "        ax.plot([orig[0], orig[0]], [orig[1], anon[1]], [orig[2], anon[2]], color='gray', linestyle='--', linewidth=0.5)\n",
    "        ax.plot([orig[0], anon[0]], [orig[1], orig[1]], [orig[2], anon[2]], color='gray', linestyle='--', linewidth=0.5)\n",
    "        ax.plot([orig[0], anon[0]], [orig[1], anon[1]], [orig[2], orig[2]], color='gray', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    ax.set_xlabel('Dimension 1')\n",
    "    ax.set_ylabel('Dimension 2')\n",
    "    ax.set_zlabel('Dimension 3')\n",
    "    ax.set_title(title)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def visualize_embeddings(original_embeddings, anonymized_embeddings, title='Embeddings Visualization'):\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Plot original embeddings in blue\n",
    "    ax.scatter(original_embeddings[:, 0], original_embeddings[:, 1], original_embeddings[:, 2], c='b', marker='o', label='Original Embeddings', s=50)\n",
    "\n",
    "    # Plot anonymized embeddings in red\n",
    "    ax.scatter(anonymized_embeddings[:, 0], anonymized_embeddings[:, 1], anonymized_embeddings[:, 2], c='r', marker='o', label='Anonymized Embeddings')\n",
    "\n",
    "    # Connect each original point to its corresponding anonymized point\n",
    "    for orig, anon in zip(original_embeddings, anonymized_embeddings):\n",
    "        ax.plot([orig[0], anon[0]], [orig[1], anon[1]], [orig[2], anon[2]], color='gray', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    ax.set_xlabel('Dimension 1')\n",
    "    ax.set_ylabel('Dimension 2')\n",
    "    ax.set_zlabel('Dimension 3')\n",
    "    ax.set_title(title)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "original_embeddings = torch.randn(10, 20)  # Replace with your actual embeddings\n",
    "anonymized_embeddings = anonymize_embeddings_pca(original_embeddings, n_components=10)\n",
    "\n",
    "similar_embeddings = (original_embeddings + torch.tensor(np.random.normal(scale=0.01, size=original_embeddings.shape)))\n",
    "\n",
    "# Save PCA model for reconstruction\n",
    "pca_model = PCA().fit(similar_embeddings.numpy())\n",
    "\n",
    "\n",
    "#TODO: Use CIFAR10 and CIFAR10H (noisy CIFAR10) to check reconstruction\n",
    "\n",
    "# Reconstruct the embeddings\n",
    "reconstructed_embeddings = reconstruct_embeddings_pca(anonymized_embeddings, pca_model)\n",
    "\n",
    "# Check if reconstruction is close to the original\n",
    "#print(torch.allclose(similar_embeddings, reconstructed_embeddings, rtol=1e-03, atol=1e-03))\n",
    "\n",
    "# Visualize both original and anonymized embeddings\n",
    "#visualize_embeddings(original_embeddings, anonymized_embeddings, title='Original vs PCA Anonymized Embeddings')\n",
    "\n",
    "# Visualize both original and anonymized embeddings\n",
    "#visualize_embeddings(reconstructed_embeddings, similar_embeddings, title='Reconstructed vs. Similar Embeddings')\n",
    "\n",
    "\n",
    "\n",
    "original_embeddings = torch.randn(50, 25)\n",
    "test_embeddings = torch.randn(10, 5)\n",
    "train_embeddings,  cluster_labels, num_clusters = anonymize_embeddings_density_based(original_embeddings, eps=50, min_samples=5, noise_scale=0.00)\n",
    "\n",
    "test_embeddings_anonymized = anonymize_embeddings_density_based_test(test_embeddings, cluster_labels, noise_scale=0.00)\n",
    "\n",
    "# Visualize both original and anonymized embeddings\n",
    "visualize_embeddings2(test_embeddings, test_embeddings_anonymized, title='Test and Test Anonymized Embeddings')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Visualize both original and anonymized embeddings\n",
    "visualize_embeddings2(original_embeddings, anonymized_embeddings, title='Original and Anonymized Embeddings')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
